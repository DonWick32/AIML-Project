{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=3)\n",
    "\n",
    "def evaluate(board):\n",
    "    \"\"\"\n",
    "    Evaluate the current state of the board.\n",
    "    Returns:\n",
    "    - 1 if the maximizing player wins\n",
    "    - -1 if the minimizing player wins\n",
    "    - 0 if it's a tie or the game is ongoing\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(len(board)))\n",
    "\n",
    "    # Check rows and columns\n",
    "    for i in range(n):\n",
    "        if all(board[i * n + j] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[j * n + i] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[i * n + j] == 2 for j in range(n)):\n",
    "            return -1\n",
    "        if all(board[j * n + i] == 2 for j in range(n)):\n",
    "            return -1\n",
    "\n",
    "    # Check diagonals\n",
    "    if all(board[i * (n + 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n + 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "\n",
    "    # Check for a tie\n",
    "    if all(cell != 0 for cell in board):\n",
    "        return 0\n",
    "\n",
    "    # Game still ongoing\n",
    "    return None\n",
    "\n",
    "def abminimax(board, depth, alpha, beta, maximizingPlayer):\n",
    "    \"\"\"\n",
    "    Implementation of the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    score = evaluate(board)\n",
    "    # print(score)\n",
    "\n",
    "    if score is not None:\n",
    "        return score\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        maxEval = -np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 1\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, False)\n",
    "                maxEval = max(maxEval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return maxEval\n",
    "    else:\n",
    "        minEval = np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 2\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, True)\n",
    "                minEval = min(minEval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return minEval\n",
    "\n",
    "def find_best_move(board):\n",
    "    \"\"\"\n",
    "    Finds the best move using the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    print(board)\n",
    "    best_val = -np.inf\n",
    "    best_move = None\n",
    "    alpha = -np.inf\n",
    "    beta = np.inf\n",
    "\n",
    "    for i in range(len(board)):\n",
    "        if board[i] == 0:\n",
    "            new_board = copy.deepcopy(board)\n",
    "            new_board[i] = 1\n",
    "            move_val = abminimax(new_board, 0, alpha, beta, False)\n",
    "\n",
    "            if move_val > best_val:\n",
    "                best_move = i\n",
    "                best_val = move_val\n",
    "            alpha = max(alpha, move_val)\n",
    "\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's turn!\n",
      "   | | \n",
      "  ------\n",
      "   | | \n",
      "  ------\n",
      "   |O| \n",
      "Alpha-Beta's turn!\n",
      "[0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
      "   |X| \n",
      "  ------\n",
      "   | | \n",
      "  ------\n",
      "   |O| \n",
      "PPO's turn!\n",
      "   |X| \n",
      "  ------\n",
      "   | | \n",
      "  ------\n",
      "   |O|O\n",
      "Alpha-Beta's turn!\n",
      "[0. 1. 0. 0. 0. 0. 0. 2. 2.]\n",
      "   |X| \n",
      "  ------\n",
      "   | | \n",
      "  ------\n",
      "  X|O|O\n",
      "PPO's turn!\n",
      "   |X| \n",
      "  ------\n",
      "   | |O\n",
      "  ------\n",
      "  X|O|O\n",
      "Alpha-Beta's turn!\n",
      "[0. 1. 0. 0. 0. 2. 1. 2. 2.]\n",
      "   |X|X\n",
      "  ------\n",
      "   | |O\n",
      "  ------\n",
      "  X|O|O\n",
      "PPO's turn!\n",
      "   |X|X\n",
      "  ------\n",
      "   |O|O\n",
      "  ------\n",
      "  X|O|O\n",
      "Alpha-Beta's turn!\n",
      "[0. 1. 1. 0. 2. 2. 1. 2. 2.]\n",
      "  X|X|X\n",
      "  ------\n",
      "   |O|O\n",
      "  ------\n",
      "  X|O|O\n",
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the minimax algorithm with the TicTacToe environment\n",
    "from stable_baselines3 import DQN, DDPG, PPO\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "model = PPO.load(\"ppomodel/best_model.zip\")\n",
    "observation = np.asarray([0] * 9)\n",
    "while not done:\n",
    "    # Human player's turn\n",
    "    print(\"PPO's turn!\")\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, trunc, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        break\n",
    "    \n",
    "    # Agent's turn using minimax\n",
    "    print(\"Alpha-Beta's turn!\")\n",
    "    best_move = np.asarray(find_best_move(observation))\n",
    "    observation, reward, done, _, _ = env.step(best_move)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of how to use the minimax algorithm with the TicTacToe environment\n",
    "# env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     # Human player's turn\n",
    "#     # env.render()\n",
    "#     print(\"Your turn!\")\n",
    "#     n = env.size\n",
    "#     action = np.asarray(int(input(\"Enter action: \")))\n",
    "#     observation, reward, done, _, _ = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "#     if done:\n",
    "#         print(\"Game Over!\")\n",
    "#         break\n",
    "\n",
    "#     # Agent's turn using minimax\n",
    "#     print(\"Agent's turn!\")\n",
    "#     print(observation)\n",
    "#     best_move = np.asarray(find_best_move(observation))\n",
    "#     observation, reward, done, _, _ = env.step(best_move)\n",
    "    \n",
    "#     env.render()\n",
    "    \n",
    "#     if done:\n",
    "#         print(\"Game Over!\")\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
