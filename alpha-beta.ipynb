{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG VS MIN MAX SIZE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=3)\n",
    "\n",
    "def evaluate(board):\n",
    "    \"\"\"\n",
    "    Evaluate the current state of the board.\n",
    "    Returns:\n",
    "    - 1 if the maximizing player wins\n",
    "    - -1 if the minimizing player wins\n",
    "    - 0 if it's a tie or the game is ongoing\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(len(board)))\n",
    "\n",
    "    # Check rows and columns\n",
    "    for i in range(n):\n",
    "        if all(board[i * n + j] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[j * n + i] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[i * n + j] == 2 for j in range(n)):\n",
    "            return -1\n",
    "        if all(board[j * n + i] == 2 for j in range(n)):\n",
    "            return -1\n",
    "\n",
    "    # Check diagonals\n",
    "    if all(board[i * (n + 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n + 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "\n",
    "    # Check for a tie\n",
    "    if all(cell != 0 for cell in board):\n",
    "        return 0\n",
    "\n",
    "    # Game still ongoing\n",
    "    return None\n",
    "\n",
    "def abminimax(board, depth, alpha, beta, maximizingPlayer):\n",
    "    \"\"\"\n",
    "    Implementation of the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    score = evaluate(board)\n",
    "    # print(score)\n",
    "\n",
    "    if score is not None:\n",
    "        return score\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        maxEval = -np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 1\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, False)\n",
    "                maxEval = max(maxEval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return maxEval\n",
    "    else:\n",
    "        minEval = np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 2\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, True)\n",
    "                minEval = min(minEval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return minEval\n",
    "\n",
    "def find_best_move(board):\n",
    "    \"\"\"\n",
    "    Finds the best move using the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    print(board)\n",
    "    best_val = -np.inf\n",
    "    best_move = None\n",
    "    alpha = -np.inf\n",
    "    beta = np.inf\n",
    "\n",
    "    for i in range(len(board)):\n",
    "        if board[i] == 0:\n",
    "            new_board = copy.deepcopy(board)\n",
    "            new_board[i] = 1\n",
    "            move_val = abminimax(new_board, 0, alpha, beta, False)\n",
    "\n",
    "            if move_val > best_val:\n",
    "                best_move = i\n",
    "                best_val = move_val\n",
    "            alpha = max(alpha, move_val)\n",
    "\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG's turn!\n",
      "Time taken:  0.001592397689819336\n",
      "   | | \n",
      "  ------\n",
      "   | | \n",
      "  ------\n",
      "   |O| \n",
      "Alpha-Beta's turn!\n",
      "[0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
      "Time taken:  0.3490467071533203\n",
      "   |X| \n",
      "  ------\n",
      "   | | \n",
      "  ------\n",
      "   |O| \n",
      "DDPG's turn!\n",
      "Time taken:  0.0019240379333496094\n",
      "   |X| \n",
      "  ------\n",
      "   | |O\n",
      "  ------\n",
      "   |O| \n",
      "Alpha-Beta's turn!\n",
      "[0. 1. 0. 0. 0. 2. 0. 2. 0.]\n",
      "Time taken:  0.03490400314331055\n",
      "   |X| \n",
      "  ------\n",
      "   | |O\n",
      "  ------\n",
      "  X|O| \n",
      "DDPG's turn!\n",
      "Time taken:  0.0021255016326904297\n",
      "   |X|O\n",
      "  ------\n",
      "   | |O\n",
      "  ------\n",
      "  X|O| \n",
      "Alpha-Beta's turn!\n",
      "[0. 1. 2. 0. 0. 2. 1. 2. 0.]\n",
      "Time taken:  0.0027532577514648438\n",
      "   |X|O\n",
      "  ------\n",
      "   | |O\n",
      "  ------\n",
      "  X|O|X\n",
      "DDPG's turn!\n",
      "Time taken:  0.0030155181884765625\n",
      "   |X|O\n",
      "  ------\n",
      "   |O|O\n",
      "  ------\n",
      "  X|O|X\n",
      "Alpha-Beta's turn!\n",
      "[0. 1. 2. 0. 2. 2. 1. 2. 1.]\n",
      "Time taken:  0.0004611015319824219\n",
      "   |X|O\n",
      "  ------\n",
      "  X|O|O\n",
      "  ------\n",
      "  X|O|X\n",
      "DDPG's turn!\n",
      "Time taken:  0.0011348724365234375\n",
      "  O|X|O\n",
      "  ------\n",
      "  X|O|O\n",
      "  ------\n",
      "  X|O|X\n",
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the minimax algorithm with the TicTacToe environment\n",
    "from stable_baselines3 import DQN, DDPG, PPO\n",
    "import time\n",
    "env.reset()\n",
    "done = False\n",
    "model = DDPG.load(\"ddpgmodel_3/best_model.zip\")\n",
    "observation = np.asarray([0] * 9)\n",
    "while not done:\n",
    "    # Human player's turn\n",
    "    print(\"DDPG's turn!\")\n",
    "    start = time.time()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    print(\"Time taken: \", time.time() - start)\n",
    "    \n",
    "    observation, reward, done, trunc, _ = env.step(np.float32(action.item()))\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        break\n",
    "    \n",
    "    # Agent's turn using minimax\n",
    "    \n",
    "    print(\"Alpha-Beta's turn!\")\n",
    "    start = time.time()\n",
    "    best_move = np.asarray(find_best_move(observation))\n",
    "    print(\"Time taken: \", time.time() - start)\n",
    "    observation, reward, done, _, _ = env.step(np.float32(best_move.item()))\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG VS RANDOM size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670c9ae8af84474e83f173b37cc8dc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG Time taken:  0.002591557502746582\n",
      "DDPG won:  83 Lost:  17 Tie:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=3)\n",
    "\n",
    "def evaluate(board):\n",
    "    \"\"\"\n",
    "    Evaluate the current state of the board.\n",
    "    Returns:\n",
    "    - 1 if the maximizing player wins\n",
    "    - -1 if the minimizing player wins\n",
    "    - 0 if it's a tie or the game is ongoing\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(len(board)))\n",
    "\n",
    "    # Check rows and columns\n",
    "    for i in range(n):\n",
    "        if all(board[i * n + j] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[j * n + i] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[i * n + j] == 2 for j in range(n)):\n",
    "            return -1\n",
    "        if all(board[j * n + i] == 2 for j in range(n)):\n",
    "            return -1\n",
    "\n",
    "    # Check diagonals\n",
    "    if all(board[i * (n + 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n + 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "\n",
    "    # Check for a tie\n",
    "    if all(cell != 0 for cell in board):\n",
    "        return 0\n",
    "\n",
    "    # Game still ongoing\n",
    "    return None\n",
    "\n",
    "def abminimax(board, depth, alpha, beta, maximizingPlayer):\n",
    "    \"\"\"\n",
    "    Implementation of the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    score = evaluate(board)\n",
    "    # print(score)\n",
    "\n",
    "    if score is not None:\n",
    "        return score\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        maxEval = -np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 1\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, False)\n",
    "                maxEval = max(maxEval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return maxEval\n",
    "    else:\n",
    "        minEval = np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 2\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, True)\n",
    "                minEval = min(minEval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return minEval\n",
    "\n",
    "def find_best_move(board):\n",
    "    \"\"\"\n",
    "    Finds the best move using the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    print(board)\n",
    "    best_val = -np.inf\n",
    "    best_move = None\n",
    "    alpha = -np.inf\n",
    "    beta = np.inf\n",
    "\n",
    "    for i in range(len(board)):\n",
    "        if board[i] == 0:\n",
    "            new_board = copy.deepcopy(board)\n",
    "            new_board[i] = 1\n",
    "            move_val = abminimax(new_board, 0, alpha, beta, False)\n",
    "\n",
    "            if move_val > best_val:\n",
    "                best_move = i\n",
    "                best_val = move_val\n",
    "            alpha = max(alpha, move_val)\n",
    "\n",
    "    return best_move\n",
    "\n",
    "\n",
    "from stable_baselines3 import DQN, DDPG, PPO\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=3)\n",
    "\n",
    "model = DDPG.load(\"ddpgmodel_3/best_model.zip\")\n",
    "\n",
    "ddpg_won, random_won = 0, 0 \n",
    "time_taken = 0\n",
    "n = 100\n",
    "observation = np.asarray([0] * 9)\n",
    "for iter in tqdm(range(n)):\n",
    "    done = False\n",
    "    env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Human player's turn\n",
    "        # print(\"DDPG's turn!\")\n",
    "        \n",
    "        start = time.time()\n",
    "        action, _states = model.predict(observation, deterministic=True)\n",
    "        # print(\"Time taken: \", time.time() - start)\n",
    "        time_taken += time.time() - start\n",
    "        observation, reward_ddpg, done, trunc, _ = env.step(np.float32(action.item()))\n",
    "        # env.render()\n",
    "        # print(\"\\n\\n\")\n",
    "    \n",
    "        if done:\n",
    "            # print(reward)\n",
    "            ddpg_won+=1\n",
    "            # print(\"Game Over!\")\n",
    "            break\n",
    "        \n",
    "        # Agent's turn using minimax\n",
    "\n",
    "        # print(\"Random's turn!\")\n",
    "        random_move = np.random.randint(0, 9)\n",
    "        observation, reward, done, _, _ = env.step(np.float32(random_move))\n",
    "\n",
    "        if done:\n",
    "            random_won+=1\n",
    "            # print(\"Game Over!\")\n",
    "            break\n",
    "        \n",
    "    # print(\"===\\n\\n\", done, reward, reward_ddpg, trunc)\n",
    "    # env.render()\n",
    "\n",
    "print(\"AVG Time taken: \", time_taken / n)\n",
    "print(\"DDPG won: \", ddpg_won, \"Lost: \", random_won, \"Tie: \", n - ddpg_won - random_won)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG VS MIN MAX size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=4)\n",
    "\n",
    "def evaluate(board):\n",
    "    \"\"\"\n",
    "    Evaluate the current state of the board.\n",
    "    Returns:\n",
    "    - 1 if the maximizing player wins\n",
    "    - -1 if the minimizing player wins\n",
    "    - 0 if it's a tie or the game is ongoing\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(len(board)))\n",
    "\n",
    "    # Check rows and columns\n",
    "    for i in range(n):\n",
    "        if all(board[i * n + j] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[j * n + i] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[i * n + j] == 2 for j in range(n)):\n",
    "            return -1\n",
    "        if all(board[j * n + i] == 2 for j in range(n)):\n",
    "            return -1\n",
    "\n",
    "    # Check diagonals\n",
    "    if all(board[i * (n + 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n + 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "\n",
    "    # Check for a tie\n",
    "    if all(cell != 0 for cell in board):\n",
    "        return 0\n",
    "\n",
    "    # Game still ongoing\n",
    "    return None\n",
    "\n",
    "def abminimax(board, depth, alpha, beta, maximizingPlayer):\n",
    "    \"\"\"\n",
    "    Implementation of the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    score = evaluate(board)\n",
    "    # print(score)\n",
    "\n",
    "    if score is not None:\n",
    "        return score\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        maxEval = -np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 1\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, False)\n",
    "                maxEval = max(maxEval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return maxEval\n",
    "    else:\n",
    "        minEval = np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 2\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, True)\n",
    "                minEval = min(minEval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return minEval\n",
    "\n",
    "def find_best_move(board):\n",
    "    \"\"\"\n",
    "    Finds the best move using the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    print(board)\n",
    "    best_val = -np.inf\n",
    "    best_move = None\n",
    "    alpha = -np.inf\n",
    "    beta = np.inf\n",
    "\n",
    "    for i in range(len(board)):\n",
    "        if board[i] == 0:\n",
    "            new_board = copy.deepcopy(board)\n",
    "            new_board[i] = 1\n",
    "            move_val = abminimax(new_board, 0, alpha, beta, False)\n",
    "\n",
    "            if move_val > best_val:\n",
    "                best_move = i\n",
    "                best_val = move_val\n",
    "            alpha = max(alpha, move_val)\n",
    "\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG's turn!\n",
      "Time taken:  0.0022487640380859375\n",
      "   | | | \n",
      "  --------\n",
      "   | | | \n",
      "  --------\n",
      "   | |O| \n",
      "  --------\n",
      "   | | | \n",
      "Alpha-Beta's turn!\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAlpha-Beta\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms turn!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m best_move \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(find_best_move(observation))\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTime taken: \u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n\u001b[1;32m     28\u001b[0m observation, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(np\u001b[39m.\u001b[39mfloat32(best_move\u001b[39m.\u001b[39mitem()))\n",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36mfind_best_move\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m     94\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     95\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 96\u001b[0m move_val \u001b[39m=\u001b[39m abminimax(new_board, \u001b[39m0\u001b[39;49m, alpha, beta, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m move_val \u001b[39m>\u001b[39m best_val:\n\u001b[1;32m     99\u001b[0m     best_move \u001b[39m=\u001b[39m i\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     73\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     74\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m abminimax(new_board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, alpha, beta, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     76\u001b[0m minEval \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(minEval, \u001b[39meval\u001b[39m)\n\u001b[1;32m     77\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(beta, \u001b[39meval\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     61\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     62\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m abminimax(new_board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, alpha, beta, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     64\u001b[0m maxEval \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(maxEval, \u001b[39meval\u001b[39m)\n\u001b[1;32m     65\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(alpha, \u001b[39meval\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     73\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     74\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m abminimax(new_board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, alpha, beta, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     76\u001b[0m minEval \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(minEval, \u001b[39meval\u001b[39m)\n\u001b[1;32m     77\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(beta, \u001b[39meval\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     61\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     62\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m abminimax(new_board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, alpha, beta, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     64\u001b[0m maxEval \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(maxEval, \u001b[39meval\u001b[39m)\n\u001b[1;32m     65\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(alpha, \u001b[39meval\u001b[39m)\n",
      "    \u001b[0;31m[... skipping similar frames: abminimax at line 75 (4 times), abminimax at line 63 (4 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     73\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     74\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m abminimax(new_board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, alpha, beta, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     76\u001b[0m minEval \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(minEval, \u001b[39meval\u001b[39m)\n\u001b[1;32m     77\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(beta, \u001b[39meval\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     61\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     62\u001b[0m new_board[i] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m abminimax(new_board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, alpha, beta, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     64\u001b[0m maxEval \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(maxEval, \u001b[39meval\u001b[39m)\n\u001b[1;32m     65\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(alpha, \u001b[39meval\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mabminimax\u001b[0;34m(board, depth, alpha, beta, maximizingPlayer)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mabminimax\u001b[39m(board, depth, alpha, beta, maximizingPlayer):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    Implementation of the alpha-beta pruning minimax algorithm.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     score \u001b[39m=\u001b[39m evaluate(board)\n\u001b[1;32m     52\u001b[0m     \u001b[39m# print(score)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m score \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(board[i \u001b[39m*\u001b[39m n \u001b[39m+\u001b[39m j] \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)):\n\u001b[1;32m     26\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39;49m(board[j \u001b[39m*\u001b[39;49m n \u001b[39m+\u001b[39;49m i] \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39mfor\u001b[39;49;00m j \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n)):\n\u001b[1;32m     28\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39m# Check diagonals\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of how to use the minimax algorithm with the TicTacToe environment\n",
    "from stable_baselines3 import DQN, DDPG, PPO\n",
    "import time\n",
    "env.reset()\n",
    "done = False\n",
    "model = DDPG.load(\"ddpgmodel_4/best_model.zip\")\n",
    "observation = np.asarray([0] * 16)\n",
    "while not done:\n",
    "    # Human player's turn\n",
    "    print(\"DDPG's turn!\")\n",
    "    start = time.time()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    print(\"Time taken: \", time.time() - start)\n",
    "    \n",
    "    observation, reward, done, trunc, _ = env.step(np.float32(action.item()))\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        break\n",
    "    \n",
    "    # Agent's turn using minimax\n",
    "    \n",
    "    print(\"Alpha-Beta's turn!\")\n",
    "    start = time.time()\n",
    "    best_move = np.asarray(find_best_move(observation))\n",
    "    print(\"Time taken: \", time.time() - start)\n",
    "    observation, reward, done, _, _ = env.step(np.float32(best_move.item()))\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Game Over!\")\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623a14a3d48f478bb357f9990772d5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG Time taken:  0.003969688415527344\n",
      "DDPG won:  69 Lost:  31 Tie:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=4)\n",
    "\n",
    "def evaluate(board):\n",
    "    \"\"\"\n",
    "    Evaluate the current state of the board.\n",
    "    Returns:\n",
    "    - 1 if the maximizing player wins\n",
    "    - -1 if the minimizing player wins\n",
    "    - 0 if it's a tie or the game is ongoing\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(len(board)))\n",
    "\n",
    "    # Check rows and columns\n",
    "    for i in range(n):\n",
    "        if all(board[i * n + j] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[j * n + i] == 1 for j in range(n)):\n",
    "            return 1\n",
    "        if all(board[i * n + j] == 2 for j in range(n)):\n",
    "            return -1\n",
    "        if all(board[j * n + i] == 2 for j in range(n)):\n",
    "            return -1\n",
    "\n",
    "    # Check diagonals\n",
    "    if all(board[i * (n + 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 1 for i in range(n)):\n",
    "        return 1\n",
    "    if all(board[i * (n + 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "    if all(board[i * (n - 1) + (n - 1)] == 2 for i in range(n)):\n",
    "        return -1\n",
    "\n",
    "    # Check for a tie\n",
    "    if all(cell != 0 for cell in board):\n",
    "        return 0\n",
    "\n",
    "    # Game still ongoing\n",
    "    return None\n",
    "\n",
    "def abminimax(board, depth, alpha, beta, maximizingPlayer):\n",
    "    \"\"\"\n",
    "    Implementation of the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    score = evaluate(board)\n",
    "    # print(score)\n",
    "\n",
    "    if score is not None:\n",
    "        return score\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        maxEval = -np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 1\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, False)\n",
    "                maxEval = max(maxEval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return maxEval\n",
    "    else:\n",
    "        minEval = np.inf\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == 0:\n",
    "                new_board = copy.deepcopy(board)\n",
    "                new_board[i] = 2\n",
    "                eval = abminimax(new_board, depth + 1, alpha, beta, True)\n",
    "                minEval = min(minEval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "        return minEval\n",
    "\n",
    "def find_best_move(board):\n",
    "    \"\"\"\n",
    "    Finds the best move using the alpha-beta pruning minimax algorithm.\n",
    "    \"\"\"\n",
    "    print(board)\n",
    "    best_val = -np.inf\n",
    "    best_move = None\n",
    "    alpha = -np.inf\n",
    "    beta = np.inf\n",
    "\n",
    "    for i in range(len(board)):\n",
    "        if board[i] == 0:\n",
    "            new_board = copy.deepcopy(board)\n",
    "            new_board[i] = 1\n",
    "            move_val = abminimax(new_board, 0, alpha, beta, False)\n",
    "\n",
    "            if move_val > best_val:\n",
    "                best_move = i\n",
    "                best_val = move_val\n",
    "            alpha = max(alpha, move_val)\n",
    "\n",
    "    return best_move\n",
    "\n",
    "\n",
    "from stable_baselines3 import DQN, DDPG, PPO\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gym_tictactoe.env import TicTacToeEnv\n",
    "\n",
    "\n",
    "env = TicTacToeEnv(size=4)\n",
    "\n",
    "model = DDPG.load(\"ddpgmodel_4/best_model.zip\")\n",
    "\n",
    "ddpg_won, random_won = 0, 0 \n",
    "time_taken = 0\n",
    "n = 100\n",
    "for iter in tqdm(range(n)):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    observation = np.asarray([0] * 16)\n",
    "    while not done:\n",
    "        # Human player's turn\n",
    "        # print(\"DDPG's turn!\")\n",
    "        \n",
    "        start = time.time()\n",
    "        action, _states = model.predict(observation, deterministic=True)\n",
    "        # print(\"Time taken: \", time.time() - start)\n",
    "        time_taken += time.time() - start\n",
    "        observation, reward_ddpg, done, trunc, _ = env.step(np.float32(action.item()))\n",
    "        # env.render()\n",
    "        # print(\"\\n\\n\")\n",
    "    \n",
    "        if done:\n",
    "            # print(reward)\n",
    "            random_won+=1\n",
    "            # print(\"Game Over!\")\n",
    "            break\n",
    "        \n",
    "        # Agent's turn using minimax\n",
    "\n",
    "        # print(\"Random's turn!\")\n",
    "        random_move = np.random.randint(0, 9)\n",
    "        observation, reward, done, _, _ = env.step(np.float32(random_move))\n",
    "\n",
    "        if done:\n",
    "            ddpg_won+=1\n",
    "            # print(\"Game Over!\")\n",
    "            break\n",
    "        \n",
    "    # print(\"===\\n\\n\", done, reward, reward_ddpg, trunc)\n",
    "    # env.render()\n",
    "\n",
    "print(\"AVG Time taken: \", time_taken / n)\n",
    "print(\"DDPG won: \", ddpg_won, \"Lost: \", random_won, \"Tie: \", n - ddpg_won - random_won)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
